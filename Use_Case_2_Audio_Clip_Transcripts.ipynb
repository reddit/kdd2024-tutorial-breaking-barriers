{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83_3HgOazOPB"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Reddit, Inc.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94-aGnc7z9Da"
      },
      "source": [
        "# Use Case 2. Audio Clip Transcripts\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/reddit/kdd2024-tutorial-breaking-barriers/blob/master/Use_Case_2_Audio_Clip_Transcripts.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZOqbBWcs6J"
      },
      "source": [
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a hands-on guide to using the open-source Whisper speech-to-text model for transcribing audio clips, generating closed captions, translating transcripts, and identifying different speakers in a recording (speaker diarization). The guide includes a step-by-step walkthrough on setting up and executing transcription commands with various options. We will use Google Colab to speed up the process via their free GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQw_u05JUIH"
      },
      "source": [
        "---\n",
        "\n",
        "## Setting Up Google Colab\n",
        "Google Colab provides a convenient platform to run Python code in the cloud, with access to powerful computing resources, including GPUs. Although a GPU is not strictly necessary to use Whisper, it is recommended to enable GPU acceleration to speed up your transcriptions:\n",
        "\n",
        "1.   Click on *Runtime* in the top menu.\n",
        "2.   Select *Change runtime type*.\n",
        "3.   In the dialog that appears, under *Hardware accelerator*, choose **T4 GPU** (or any other GPU that you may have access to) if it is not already enabled.\n",
        "4.   Click *Save*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKGqwYtN_lh1"
      },
      "source": [
        "---\n",
        "\n",
        "## OpenAI Whisper\n",
        "\n",
        "[OpenAI's Whisper](https://github.com/openai/whisper) [1] is a end-to-end general-purpose speech recognition model that has been trained on a diverse dataset of 680,000 hours of multilingual and multitask supervised data collected from the web, leading to improved robustness to accents, background noise, and technical language. In addition, Whisper enables transcription in multiple languages and translation from those languages into English.\n",
        "\n",
        "Whisper has five model variants, offering speed and accuracy tradeoffs. The specific models for English language tend to perform better for English-only applications.\n",
        "\n",
        "- `large` model: ~10GB VRAM, largest model\n",
        "- `medium` and `medium.en` models: ~5GB VRAM, 2x speed\n",
        "- `small` and `small.en` models: ~2GB VRAM, 6x speed\n",
        "- `base` and `base.en` models: ~1GB VRAM, 16x speed\n",
        "- `tiny` and `tiny.en` models: ~1GB VRAM, 32x speed\n",
        "\n",
        "Models were released on September 2022 (original series), December 2022 (large-v2), and November 2023 (large-v3).\n",
        "\n",
        "Whisper supports 99 different languages and performance varies widely depending on the language.\n",
        "\n",
        "##### **References**\n",
        "[1] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak Supervision. [arXiv:2212.04356](https://arxiv.org/abs/2212.04356) [eess.AS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoEv_t8AIy3_"
      },
      "source": [
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Run the following cell to install the required Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfk1GOivIz8V"
      },
      "outputs": [],
      "source": [
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Settings"
      ],
      "metadata": {
        "id": "ESq2tU585PPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to get the run time on every cell execution:"
      ],
      "metadata": {
        "id": "6tnCJy0K5RIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "L4Wd6nzb5QNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to enable wrap when printing long strings:"
      ],
      "metadata": {
        "id": "Jw2K-yTY5c3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "WRHNZm-_5dPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSoXTovI81hc"
      },
      "source": [
        "---\n",
        "\n",
        "## Test Videos\n",
        "\n",
        "We will use the two first videos in the tutorial dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7m1ymeh9pzb"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/reddit/kdd2024-tutorial-breaking-barriers/main/media/video1.mp4\"\n",
        "!wget \"https://raw.githubusercontent.com/reddit/kdd2024-tutorial-breaking-barriers/main/media/video2.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vaRMOx6821H"
      },
      "outputs": [],
      "source": [
        "import moviepy.editor\n",
        "moviepy.editor.ipython_display(\"video1.mp4\", height=400, maxduration=300)\n",
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Using Command Line"
      ],
      "metadata": {
        "id": "6wWkpe6ETERq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f18DAHnYLz-T"
      },
      "source": [
        "### Transcribing Audio\n",
        "\n",
        "We can run Whisper through command-line arguments.\n",
        "\n",
        "Let's first test whisper through the `whisper --help` command-line argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUiNiuLUL49j"
      },
      "outputs": [],
      "source": [
        "!whisper --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQOKuY5wMBuM"
      },
      "source": [
        "Let's now try and transcribe an audio file. The following command will transcribe speech in our test audio file. We will use the arguments\n",
        "\n",
        "1.   `--model base`: specify the model size\n",
        "2.   `--task transcribe`: to do transcription\n",
        "3.   `--output_dir transcription`: to save the files in the directory transcription\n",
        "4.   `--output_format all`: give transcription in all formats. Otherwise, you may select the one your prefer.\n",
        "\n",
        "Some hints:\n",
        "- If you are running whisper without a GPU, you should include `--device cpu` in the command\n",
        "- You can do multiple audio files at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHRveijVMOGO"
      },
      "outputs": [],
      "source": [
        "!whisper video1.mp4 --model base --task transcribe --output_dir transcription --output_format all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkO7dIARMzP5"
      },
      "source": [
        "The `transcription` folder has now some files with the transcripts in different formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKjempEHNXoa"
      },
      "source": [
        "### Generating Closed Captions\n",
        "\n",
        "It is very easy to use the closed captions in SRT format (standard SubRip format for video captioning) to insert them back into the video as a separate subtitle track or burnt into the video frames, for instance using ffmpeg tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY0j2XHfQaI0"
      },
      "outputs": [],
      "source": [
        "# Subtitle track:\n",
        "!ffmpeg -i video1.mp4 -i transcription/video1.srt -c copy -c:s mov_text subtitled1_video1.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtlyaBdhQi3_"
      },
      "outputs": [],
      "source": [
        "# Video frames\n",
        "!ffmpeg -i video1.mp4 -vf subtitles=transcription/video1.srt subtitled2_video1.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRUvMTQ_NGTN"
      },
      "source": [
        "### Translating Audio\n",
        "\n",
        "If your audio is in a language different than English, Whisper provides the choice to translate it to English, with the `translate` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7BhLp0mNMry"
      },
      "outputs": [],
      "source": [
        "!whisper video2.mp4 --model base --task translate --output_dir transcription --output_format all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e0R6gVhI6g8"
      },
      "source": [
        "---\n",
        "\n",
        "## Expert Mode (Using Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcribing Audio"
      ],
      "metadata": {
        "id": "kaeB1E0ETBFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgNBEOU6IjEV"
      },
      "outputs": [],
      "source": [
        "# Import package\n",
        "import whisper\n",
        "\n",
        "# Load model - choose the most appropriate\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe audio/video\n",
        "transcript = model.transcribe(\"video1.mp4\")\n",
        "\n",
        "# Print result\n",
        "print(transcript[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all info\n",
        "transcript"
      ],
      "metadata": {
        "id": "DaufsLCVTjS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translating Audio"
      ],
      "metadata": {
        "id": "-9GnGLlPULCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original language\n",
        "transcript = model.transcribe(\"video2.mp4\")\n",
        "print(transcript[\"text\"])\n",
        "\n",
        "# Translated to English\n",
        "options = dict(task=\"translate\")\n",
        "transcript = model.transcribe(\"video2.mp4\", **options)\n",
        "print(transcript[\"text\"])"
      ],
      "metadata": {
        "id": "gEIhAPj1UMol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQx6lcK0n5Oc"
      },
      "source": [
        "---\n",
        "\n",
        "## Pro Mode\n",
        "\n",
        "The following implementation is directly based on Transformers and uses some optimizations to speed up the transcription, based on the ideas in [insanely-fast-whisper](https://github.com/Vaibhavs10/insanely-fast-whisper).\n",
        "\n",
        "Any of these models may be used: https://huggingface.co/models?search=openai%2Fwhisper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "-yeqjKnuW6zv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My57tbNhn57n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "class SpeechToText:\n",
        "    CHUNK_LENGTH = 30\n",
        "    BATCH_SIZE = 24\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"Model initialization.\"\"\"\n",
        "        self.pipeline = pipeline(\n",
        "            \"automatic-speech-recognition\",\n",
        "            model_path,\n",
        "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "    def from_media_url(\n",
        "        self,\n",
        "        media_url: str,\n",
        "        language: str = None,\n",
        "        translate: bool = False,\n",
        "        include_timestamps: bool = True,\n",
        "        silence: float = 3.0,\n",
        "    ):\n",
        "        local_media_file = SpeechToText._download_media(media_url)\n",
        "        return self.from_local_file(\n",
        "            local_media_file, language, translate, include_timestamps, silence\n",
        "        )\n",
        "\n",
        "    def from_local_file(\n",
        "        self,\n",
        "        local_media_file: Path,\n",
        "        language: str = None,\n",
        "        translate: bool = False,\n",
        "        include_timestamps: bool = True,\n",
        "        silence: float = 3.0,\n",
        "    ):\n",
        "        generate_kwargs = {}\n",
        "        if translate:\n",
        "            generate_kwargs[\"task\"] = \"translate\"\n",
        "        if language:\n",
        "            generate_kwargs[\"language\"] = language\n",
        "        transcript = self.pipeline(\n",
        "            SpeechToText._get_audio(local_media_file),\n",
        "            generate_kwargs=generate_kwargs if generate_kwargs else None,\n",
        "            chunk_length_s=self.CHUNK_LENGTH,\n",
        "            batch_size=self.BATCH_SIZE,\n",
        "            return_timestamps=True,\n",
        "            return_language=True,\n",
        "        )\n",
        "        # generate output\n",
        "        response = {\n",
        "            \"language\": transcript[\"chunks\"][0][\"language\"],\n",
        "            \"text\": SpeechToText._extract_text(transcript, silence, include_timestamps),\n",
        "            \"srt\": SpeechToText._extract_srt(transcript),\n",
        "        }\n",
        "        return response\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_text(transcript: dict, silence: float, include_timestamps: bool):\n",
        "        \"\"\"Extract text chunks from transcript (separated by silence, with/out timestamps).\"\"\"\n",
        "        result = \"\"\n",
        "        prev_end = -1\n",
        "        for chunk in transcript[\"chunks\"]:\n",
        "            (start, end) = chunk[\"timestamp\"]\n",
        "            text = chunk[\"text\"]\n",
        "            if prev_end == -1:\n",
        "                if include_timestamps:\n",
        "                    result += \"{}-{}\".format(\n",
        "                        SpeechToText._seconds_to_time_format(0.0, start)[1],\n",
        "                        SpeechToText._seconds_to_time_format(0.0, end)[1],\n",
        "                    )\n",
        "                    # result += f\"{start:.2f}-{end:.2f}\"\n",
        "            elif prev_end != -1 and start >= prev_end + silence:\n",
        "                result += \"\\n\"\n",
        "                if include_timestamps:\n",
        "                    result += \"{}-{}\".format(\n",
        "                        SpeechToText._seconds_to_time_format(0.0, start)[1],\n",
        "                        SpeechToText._seconds_to_time_format(0.0, end)[1],\n",
        "                    )\n",
        "                    # result += f\"{start:.2f}-{end:.2f}\"\n",
        "            result += text\n",
        "            prev_end = end\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_srt(transcript: dict) -> str:\n",
        "        \"\"\"Extract srt format from transcript.\"\"\"\n",
        "        srt = []\n",
        "        prev = 0.0\n",
        "        for index, chunk in enumerate(transcript[\"chunks\"]):\n",
        "            prev, start_time = SpeechToText._seconds_to_time_format(\n",
        "                prev, chunk[\"timestamp\"][0]\n",
        "            )\n",
        "            prev, end_time = SpeechToText._seconds_to_time_format(\n",
        "                prev, chunk[\"timestamp\"][1]\n",
        "            )\n",
        "            srt.append(f\"{index + 1}\\n\")\n",
        "            srt.append(f\"{start_time} --> {end_time}\\n\")\n",
        "            srt.append(f\"{chunk['text'].strip()}\\n\\n\")\n",
        "        return \"\".join(srt)\n",
        "\n",
        "    @staticmethod\n",
        "    def _download_media(media_file: str, timeout: int = 5) -> str:\n",
        "        local_file = os.path.join(\"/tmp\", media_file.replace(\"/\", \"_\"))\n",
        "        with requests.get(media_file, stream=True, timeout=timeout) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_file, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        return local_file\n",
        "\n",
        "    @staticmethod\n",
        "    def _seconds_to_time_format(prev: float, seconds: float) -> any:\n",
        "        \"\"\"Convert seconds to time format.\"\"\"\n",
        "        prev = seconds\n",
        "        hours = seconds // 3600\n",
        "        seconds %= 3600\n",
        "        minutes = seconds // 60\n",
        "        seconds %= 60\n",
        "        milliseconds = int((seconds - int(seconds)) * 1000)\n",
        "        hours = int(hours)\n",
        "        minutes = int(minutes)\n",
        "        seconds = int(seconds)\n",
        "        return (\n",
        "            prev,\n",
        "            f\"{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}\",\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_audio(media_file: str) -> bytes:\n",
        "        \"\"\"Get audio data.\"\"\"\n",
        "        try:\n",
        "            ffmpeg_command = [\n",
        "                \"ffmpeg\",\n",
        "                \"-i\",\n",
        "                media_file,\n",
        "                \"-ac\",\n",
        "                \"1\",\n",
        "                \"-ar\",\n",
        "                \"16000\",\n",
        "                \"-f\",\n",
        "                \"f32le\",\n",
        "                \"-hide_banner\",\n",
        "                \"-loglevel\",\n",
        "                \"quiet\",\n",
        "                \"pipe:1\",\n",
        "            ]\n",
        "            with subprocess.Popen(\n",
        "                ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE\n",
        "            ) as ffmpeg_process:\n",
        "                output_stream = ffmpeg_process.communicate()\n",
        "        except FileNotFoundError:\n",
        "            raise \"ffmpeg was not found but is required to load audio files from filename\"\n",
        "        out_bytes = output_stream[0]\n",
        "        audio = numpy.frombuffer(out_bytes, numpy.float32)\n",
        "        if audio.shape[0] == 0:\n",
        "            raise \"this is not a valid audio\"\n",
        "        return audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "XFwrSHc-W99p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "stt = SpeechToText(\"openai/whisper-base\")"
      ],
      "metadata": {
        "id": "b8IllZUNVpgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe\n",
        "transcript = stt.from_local_file(\"video2.mp4\")\n",
        "\n",
        "# Display results\n",
        "print(transcript[\"text\"])\n",
        "print(\"######\")\n",
        "print(transcript[\"srt\"])"
      ],
      "metadata": {
        "id": "bn4UUNZLXEEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe\n",
        "transcript = stt.from_local_file(\"video2.mp4\", translate=True)\n",
        "\n",
        "# Display results\n",
        "print(transcript[\"text\"])\n",
        "print(\"######\")\n",
        "print(transcript[\"srt\"])"
      ],
      "metadata": {
        "id": "DX3O6IIRXJHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faster Whisper\n",
        "\n",
        "It is also interesting to take a look at [faster-whisper](https://github.com/SYSTRAN/faster-whisper), a reimplementation of OpenAI's Whisper model using `CTranslate2`, which is a fast inference engine for Transformer models."
      ],
      "metadata": {
        "id": "R3DTEl9lTskz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Speaker Diarization\n",
        "\n",
        "Speaker diarization is the process of identifying and segmenting speech by different speakers. Whisper itself does not support diarization but there are some initiatives to extend the model with this functionality. For instance, take a look at:\n",
        "- https://github.com/luisroque/large_laguage_models/blob/main/speech2text_whisperai_pyannotate.py\n",
        "- https://huggingface.co/pyannote/speaker-diarization-3.1"
      ],
      "metadata": {
        "id": "kTRA-OP_ZIUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Discussion: How Speech-to-Text with Multimodal LLMs can improve Accessibility in Social Media\n",
        "- **Automatic transcription of audio/video content**: Whisper can automatically transcribe audio and video posts, making them accessible to deaf and hard of hearing users.\n",
        "- **Language support**: Whisper supports multiple languages, enabling wider accessibility for a diverse user base.\n",
        "- **Reduced human effort**: Automated transcription and captioning reduce the workload on moderators, allowing them to focus on other important tasks.\n",
        "- **Improved searchability**: Transcripts generated by Whisper can be indexed and searched, making it easier for users to find relevant content.\n",
        "- **Increased engagement**: Accessible content can attract a wider audience, fostering greater engagement and participation on Reddit.\n",
        "- **Content analysis and moderation**: Transcripts can be analyzed to detect harmful or inappropriate content, improving the safety of the platform.\n"
      ],
      "metadata": {
        "id": "pE-0hqErXDVQ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
