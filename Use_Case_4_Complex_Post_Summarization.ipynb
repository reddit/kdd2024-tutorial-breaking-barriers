{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Reddit, Inc.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "J9BLmWhSehqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case 4. Complex Post Summarization\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/reddit/kdd2024-tutorial-breaking-barriers/blob/master/Use_Case_4_Complex_Post_Summarization.ipynb)"
      ],
      "metadata": {
        "id": "x7gb1HYd0O20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook guides participants to combine some of the implementations in the other use cases to show how to use AI to summarize lengthy posts for users\n",
        "with cognitive impairments. We will compare and contrast different\n",
        "summarization techniques and discuss the ethical considerations\n",
        "of using AI for summarization."
      ],
      "metadata": {
        "id": "gVq_-EtDddDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setting Up Google Colab\n",
        "Google Colab provides a convenient platform to run Python code in the cloud, with access to powerful computing resources, including GPUs. Similarly, for this tutorial, it is recommended to enable GPU acceleration:\n",
        "\n",
        "1.   Click on *Runtime* in the top menu.\n",
        "2.   Select *Change runtime type*.\n",
        "3.   In the dialog that appears, under *Hardware accelerator*, choose **T4 GPU** (or any other GPU that you may have access to) if it is not already enabled.\n",
        "4.   Click *Save*."
      ],
      "metadata": {
        "id": "93SpeOYKItV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Requirements"
      ],
      "metadata": {
        "id": "4kFfRSWgIuVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers bitsandbytes accelerate flash_attn"
      ],
      "metadata": {
        "id": "i7CFn_-JIxQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Settings\n",
        "\n",
        "Run the following cells to make some convenient settings."
      ],
      "metadata": {
        "id": "JLWu2VN9eozU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable Transformer warnings\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set GPU device\n",
        "import torch\n",
        "torch.set_default_device(\"cuda\") # or \"cpu\" is GPU is not available"
      ],
      "metadata": {
        "id": "VYNS68aYI5Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to get the run time on every cell execution:"
      ],
      "metadata": {
        "id": "HomTygaHjI46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "9P6DMHXhrkSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to enable wrap when printing long strings:"
      ],
      "metadata": {
        "id": "tCmvlRdPspjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML(\"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"))\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "metadata": {
        "id": "nJJcC9czsX0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some other useful imports:"
      ],
      "metadata": {
        "id": "SWFc2ZhgVK9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import image\n",
        "from tqdm import tqdm\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "6EzYxAVcVMhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Test Post\n",
        "\n",
        "We will use the first post in the tutorial dataset."
      ],
      "metadata": {
        "id": "h5E7DQLmKQ9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post = {\n",
        "    \"title\": \"You get me fired, so you canâ€™t work where I care about\",\n",
        "    \"body\": \"\"\"I used to work at this factory. I was a housekeeper for about a year then I transferred to the \"hoes\" department. I made the hoses for the machines. Now there was the main girl that was a team lead and she was known for being a bad person. I wouldnâ€™t have transferred but I was in an abusive relationship and was trying to save money to move. My biggest dislike of this lady was that she was VERY sexually driven. I am all for expressing yourself but she would flirt/hook up with all the guys on the line and she had a huge hang up on the big boss.\n",
        "\n",
        "She HATED that big boss was \"taking a liking\" to me. By this I mean when he asked me to do something I did it. There was this big deal about how the hose department was behind (bc they couldnâ€™t keep workers) and I was fast (even with my rib being out). She did this thing that took her an extra 10-15 mins per set which big boss told her to stop doing. She trained me to do it her way but I found it easier to do it big boss way and faster. Big boss encouraged me bc our production increased since I was there and I did what he said. (Plus I wasnâ€™t flirting with the line 50% of the day) She instantly changed towards me. Very short and rude and pointing out every mistake.\n",
        "\n",
        "This lady constantly told me that \"her\" department starts off at $17 and the job posting also said $17. So when I got $16 I was upset. I talked to boss about it and said $16 is what that department starts at and to be happy Iâ€™m making more than I was before. (Also talked to past hose girls and they said they got $16.50) After this I was pissy the rest of the day. Also my \"move out\" date was getting closer and that was also stressing me out. She was talking to me about how unfair it was that I wasnâ€™t getting paid the right amount and I said something about how big boss was a piece of shit and shouldnâ€™t even have his job (a lot more bad going on but to much to write). This angered her a lot.\n",
        "\n",
        "I leave bc I have a doc appointment and boss called me while there and texted me to come straight to office when I got back. My bosses boss was there when I walked in. They accused me of PUTTING A KNIFE TO SOMEONES NECK AND THREATENING THEM. I was shocked. Obviously I denied it bc I DID NO SUCH THING. Then it was well you put a knife to someoneâ€™s neck, then you had a knife and pointed it at them, then you had a knife and were upset. I was like \"who?? I was with lady all day. Did you ask her?\" Then they said she was the one I \"did it\" to and there were \"multiple witnesses\". Long story short they fire me. (Later found out the one \"witness\" is this ladies close, self proclaimed \"father figure\")\n",
        "\n",
        "That night my ex was extremely angry at me for losing my job then I get served DOMESTIC ABUSE papers by a cop regarding the work stuff. First off it says right on the paper this excludes coworkers. I go to court for it and she doesnâ€™t even show up!!! I move away and I did contact a lawyer about everything but he said if there are \"witnesses\" even if they arenâ€™t real that could end up having the whole thing turned against me.\n",
        "\n",
        "Now for the revenge! On my last day of work at a nursing home I see lady. She is taking to the hiring manager. She leaves and I instantly go in and ask \"was that ladyâ€™s name?\" And get told yes. I explain how she got multiple people fired at her old job and LOVES to start and stir drama. Hiring manager says she will note that. LADY DIDNT GET THE JOB ðŸ˜‚\n",
        "Footer\"\"\",\n",
        "}"
      ],
      "metadata": {
        "id": "5ZQ9YwfNJ5Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Model 1: Phi-3-small-128k-instruct\n",
        "\n",
        "The [Phi-3-small-128K-instruct](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) is another lightweight, state-of-the-art open multimodal model in the Phi-3 model family. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures."
      ],
      "metadata": {
        "id": "4c4HAEBcKsqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started"
      ],
      "metadata": {
        "id": "KzPWkQC_Mjvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-small-128k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"microsoft/Phi-3-small-128k-instruct\",\n",
        "    trust_remote_code=True)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "id": "RK7AJWi5LFCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt\n",
        "prompt = \"What about solving an 2x + 3 = 7 equation?\"\n",
        "\n",
        "# Inference\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "output = pipe(messages, **generation_args)\n",
        "result = output[0][\"generated_text\"]\n",
        "\n",
        "# Display results\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-NHuXcgtLrQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Implementation\n",
        "\n",
        "This is a more convenient implementation to prompt the model."
      ],
      "metadata": {
        "id": "Auy58Sf2MDas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor, pipeline\n",
        "\n",
        "class LLM:\n",
        "  def __init__(self):\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/Phi-3-small-128k-instruct\",\n",
        "        device_map=\"cuda\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=\"auto\")\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/Phi-3-small-128k-instruct\", trust_remote_code=True)\n",
        "\n",
        "    # Create pipeline\n",
        "    self.pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "\n",
        "  def __call__(self, prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 500,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False,\n",
        "    }\n",
        "    output = pipe(messages, **generation_args)\n",
        "    result = output[0][\"generated_text\"]\n",
        "    return result"
      ],
      "metadata": {
        "id": "u0GuK_8kMFpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "llm = LLM()"
      ],
      "metadata": {
        "id": "uky57XbaMXcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"What about solving an 2x + 3 = 7 equation?\")"
      ],
      "metadata": {
        "id": "kYivvmrVsPia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Post Summary"
      ],
      "metadata": {
        "id": "hhEQzsdtL-d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Write a short summary of the following post with a language that can be clearly understood by non-expert people\n",
        "\n",
        "## Post Title:\n",
        "{post[\"title\"]}\n",
        "\n",
        "## Post Body:\n",
        "{post[\"body\"]}\"\"\"\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "B9JN14lpMn4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Model 2: imp-v1-3b\n",
        "\n",
        "Now we are going to test the `MLLMv1` implementation (in Use Case 1. Image Short Captions) based on `imp-v1-3b` model."
      ],
      "metadata": {
        "id": "LF3hVHtZqd5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "class MLLMv1:\n",
        "\n",
        "  def __init__(self):\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    self.vision_model = AutoModelForCausalLM.from_pretrained(\n",
        "      \"MILVLG/imp-v1-3b\",\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\",\n",
        "      trust_remote_code=True)\n",
        "    self.vision_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"MILVLG/imp-v1-3b\",\n",
        "        trust_remote_code=True)\n",
        "\n",
        "  def get_image_caption(self,\n",
        "                        image: Image,\n",
        "                        base_prompt=\"Write a very short caption for the image with less than 20 words\") -> str:\n",
        "    return self.prompt_llm(image, base_prompt)\n",
        "\n",
        "  def get_image_description(self,\n",
        "                            image: Image,\n",
        "                            base_prompt=\"Write a short description for the image\") -> str:\n",
        "    return self.prompt_llm(image, base_prompt)\n",
        "\n",
        "  def prompt_llm(self,\n",
        "                 image: Image,\n",
        "                 prompt: str,\n",
        "                 max_new_tokens: int = 256,\n",
        "                 temperature: float = 0.9,\n",
        "                 top_k: int = 50,\n",
        "                 top_p: float = 0.95) -> str:\n",
        "    if image:\n",
        "      text = self.vision_tokenizer.apply_chat_template(\n",
        "          [{\"role\": \"user\", \"content\": f\"<image>\\n{prompt}\"}],\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True\n",
        "      )\n",
        "      image_tensor = self.vision_model.image_preprocess(image)\n",
        "    else:\n",
        "      text = self.vision_tokenizer.apply_chat_template(\n",
        "          [{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True\n",
        "      )\n",
        "      image_tensor = None\n",
        "    input_ids = self.vision_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "    output_ids = self.vision_model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      images=image_tensor,\n",
        "      temperature=temperature,\n",
        "      do_sample=True,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      use_cache=True)[0]\n",
        "    response = self.vision_tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    response = response.replace(\"\\n\", \" \").strip().replace(\"  \", \" \")\n",
        "    return response"
      ],
      "metadata": {
        "id": "zrVnjmSlN8DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "ic = MLLMv1()"
      ],
      "metadata": {
        "id": "CJa2g3sev70M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Write a short summary of the following post with a language that can be clearly understood by non-expert people\n",
        "\n",
        "## Post Title:\n",
        "{post[\"title\"]}\n",
        "\n",
        "## Post Body:\n",
        "{post[\"body\"]}\"\"\"\n",
        "\n",
        "print(ic.prompt_llm(None, prompt))"
      ],
      "metadata": {
        "id": "rIu9DyYlwDrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test with several prompts to improve the results."
      ],
      "metadata": {
        "id": "DU9hNtXGwWOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, generate the captions for all the posts in the provided dataset.\n",
        "- Do you find any issue?\n",
        "- Which prompt seems to work best for all posts?"
      ],
      "metadata": {
        "id": "_NaUJLf0wePT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "May you propose an approach to do multimodal post summarization, using both the post title and body and also the image or the video?"
      ],
      "metadata": {
        "id": "fgfkn0Tqw7F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Discussion: How Long Post Summarization with Multimodal LLMs can improve Accessibility in Social Media\n",
        "\n",
        "- **Concise and Accessible Summaries**: LLMs can condense lengthy social media posts into clear, concise summaries, making information easier to digest for users with cognitive impairments.\n",
        "\n",
        "- **Highlighting Key Information**: LLMs can identify and highlight crucial information, reducing cognitive overload and improving focus.\n",
        "\n",
        "- **Visual & Audio Support**: Multimodal LLMs can incorporate images, videos, and audio to enhance understanding and engagement. This is particularly helpful for users with visual or auditory processing difficulties.\n",
        "\n",
        "- **Integration with Assistive Technologies**: Seamlessly integrate LLMs with screen readers, text-to-speech software, and other assistive technologies.\n",
        "\n",
        "- **Personalized Summaries**: LLMs can tailor summaries based on individual user preferences and cognitive abilities, ensuring optimal comprehension.\n",
        "\n",
        "- **Transparency and Control**: Users should understand how the AI works and be able to control the summarization process. Allow users to customize the length, format, and level of detail in the summaries.\n",
        "\n",
        "- **Accuracy and Bias**: Consider that LLMs can sometimes generate inaccurate or biased summaries, especially when dealing with sensitive or complex topics.\n"
      ],
      "metadata": {
        "id": "srSUQAkTOOP-"
      }
    }
  ]
}